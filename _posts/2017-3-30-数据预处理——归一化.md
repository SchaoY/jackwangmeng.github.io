---
layout: post
title: 数据预处理——归一化
description: 数据预处理——归一化
category: 机器学习笔记
---


# 数据预处理——归一化

## 为什么要归一化
### 归一化后加快了梯度下降求最优解的速度
> 两个特征区间相差过大导致很难收敛甚至不能收敛。例：[1,1000]和[1，5]

### 归一化有可能提高精度
> 一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。

## 归一化方法
### 简单缩放 | min-max标准化(Min-max normalization) | 离差标准化
> #### x ＝ (x - min)/(max - min)
* max: 样本数据的最大值
* min: 为样本数据的最小值

> #### 适用场景
这种归一化方法比较适用在数值比较集中的情况。但是，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定，实际使用中可以用经验常量值来替代max和min。而且当有新数据加入时，可能导致max和min的变化，需要重新定义。
在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。

### 标准差标准化 | z-score 0均值标准化(zero-mean normalization)
> #### x = (x - u)/σ
* u: 所有样本数据的均值
* σ: 为所有样本数据的标准差。

> #### 适用场景
在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。

### 非线性归一化
> 经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。

## 使用 sklearn 做归一化
```
# built-in
import time
import operator
import collections
import datetime as dt
from functools import wraps​
# 3-party
import pandas as pd
pd.options.display.max_columns = 100
pd.options.display.max_rows = 300​

import numpy as np
import scipy
import scipy.stats​

import seaborn
from matplotlib import pylab
import matplotlib.pyplot as plt
​
import sklearn
import sklearn.preprocessing
​
from CAL.PyCAL import *

# personal lib
from lib import lib

scaler = sklearn.preprocessing.MinMaxScaler()
data = np.array(range(100))
data = data.astype(float)
data_scaler = scaler.fit_transform(data)
print list(data)
print list(data_scaler)
```
